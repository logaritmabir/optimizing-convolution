.visible .entry SM_3x3(unsigned char const*, unsigned char*, int, int)(
        .param .u64 SM_3x3(unsigned char const*, unsigned char*, int, int)_param_0,
        .param .u64 SM_3x3(unsigned char const*, unsigned char*, int, int)_param_1,
        .param .u32 SM_3x3(unsigned char const*, unsigned char*, int, int)_param_2,
        .param .u32 SM_3x3(unsigned char const*, unsigned char*, int, int)_param_3
)
{

        ld.param.u64    %rd4, [SM_3x3(unsigned char const*, unsigned char*, int, int)_param_0];
        ld.param.u64    %rd3, [SM_3x3(unsigned char const*, unsigned char*, int, int)_param_1];
        ld.param.u32    %r12, [SM_3x3(unsigned char const*, unsigned char*, int, int)_param_2];
        ld.param.u32    %r13, [SM_3x3(unsigned char const*, unsigned char*, int, int)_param_3];
        mov.u32         %r14, %ntid.x;
        mov.u32         %r15, %ctaid.x;
        mov.u32         %r1, %tid.x;
        mad.lo.s32      %r2, %r15, %r14, %r1;
        mov.u32         %r16, %ntid.y;
        mov.u32         %r17, %ctaid.y;
        mov.u32         %r3, %tid.y;
        mad.lo.s32      %r4, %r17, %r16, %r3;
        add.s32         %r5, %r1, 1;
        add.s32         %r6, %r3, 1;
        setp.ge.s32     %p1, %r4, %r12;
        setp.ge.s32     %p2, %r2, %r13;
        mad.lo.s32      %r18, %r4, %r13, %r2;
        cvt.s64.s32     %rd5, %r18;
        cvta.to.global.u64      %rd1, %rd4;
        add.s64         %rd2, %rd1, %rd5;
        mov.u32         %r19, SM_3x3(unsigned char const*, unsigned char*, int, int)::cache;
        mad.lo.s32      %r20, %r3, 18, %r19;
        add.s32         %r7, %r20, %r1;
        or.pred         %p3, %p1, %p2;
        @%p3 bra        $L__BB2_2;

        ld.global.nc.u8         %rs1, [%rd2];
        st.shared.u8    [%r7+19], %rs1;

$L__BB2_2:
        setp.ne.s32     %p4, %r3, 0;
        setp.lt.s32     %p5, %r4, 1;
        add.s32         %r8, %r19, %r5;
        or.pred         %p6, %p4, %p5;
        @%p6 bra        $L__BB2_4;

        add.s32         %r22, %r4, -1;
        mad.lo.s32      %r23, %r22, %r13, %r2;
        cvt.s64.s32     %rd6, %r23;
        add.s64         %rd7, %rd1, %rd6;
        ld.global.nc.u8         %rs2, [%rd7];
        st.shared.u8    [%r8], %rs2;

$L__BB2_4:
        setp.ne.s32     %p7, %r6, 16;
        add.s32         %r9, %r12, -1;
        setp.ge.s32     %p8, %r4, %r9;
        or.pred         %p9, %p7, %p8;
        @%p9 bra        $L__BB2_6;

        add.s32         %r24, %r4, 1;
        mad.lo.s32      %r25, %r24, %r13, %r2;
        cvt.s64.s32     %rd8, %r25;
        add.s64         %rd9, %rd1, %rd8;
        ld.global.nc.u8         %rs3, [%rd9];
        st.shared.u8    [%r8+306], %rs3;

$L__BB2_6:
        setp.ne.s32     %p10, %r1, 0;
        setp.lt.s32     %p11, %r2, 1;
        mad.lo.s32      %r10, %r6, 18, %r19;
        or.pred         %p12, %p10, %p11;
        @%p12 bra       $L__BB2_8;

        ld.global.nc.u8         %rs4, [%rd2+-1];
        st.shared.u8    [%r10], %rs4;

$L__BB2_8:
        setp.ne.s32     %p13, %r5, 16;
        add.s32         %r11, %r13, -1;
        setp.ge.s32     %p14, %r2, %r11;
        or.pred         %p15, %p13, %p14;
        @%p15 bra       $L__BB2_10;

        ld.global.nc.u8         %rs5, [%rd2+1];
        st.shared.u8    [%r10+17], %rs5;

$L__BB2_10:
        setp.lt.s32     %p16, %r2, %r11;
        bar.sync        0;
        setp.gt.s32     %p17, %r4, 0;
        setp.lt.s32     %p18, %r4, %r9;
        and.pred        %p19, %p17, %p18;
        setp.gt.s32     %p20, %r2, 0;
        and.pred        %p21, %p20, %p16;
        and.pred        %p22, %p19, %p21;
        not.pred        %p23, %p22;
        @%p23 bra       $L__BB2_12;

        ld.global.nc.f32        %f1, [GM_Filter];
        mov.u64         %rd10, GM_Filter;
        ld.shared.u8    %rs6, [%r7];
        cvt.rn.f32.u16  %f2, %rs6;
        ld.shared.u8    %rs7, [%r7+1];
        cvt.rn.f32.u16  %f3, %rs7;
        add.s64         %rd11, %rd10, 4;
        ld.global.nc.f32        %f4, [%rd11];
        mul.f32         %f5, %f4, %f3;
        fma.rn.f32      %f6, %f1, %f2, %f5;
        ld.shared.u8    %rs8, [%r7+2];
        cvt.rn.f32.u16  %f7, %rs8;
        add.s64         %rd12, %rd10, 8;
        ld.global.nc.f32        %f8, [%rd12];
        fma.rn.f32      %f9, %f8, %f7, %f6;
        ld.shared.u8    %rs9, [%r7+18];
        cvt.rn.f32.u16  %f10, %rs9;
        add.s64         %rd13, %rd10, 12;
        ld.global.nc.f32        %f11, [%rd13];
        fma.rn.f32      %f12, %f11, %f10, %f9;
        ld.shared.u8    %rs10, [%r7+19];
        cvt.rn.f32.u16  %f13, %rs10;
        add.s64         %rd14, %rd10, 16;
        ld.global.nc.f32        %f14, [%rd14];
        fma.rn.f32      %f15, %f14, %f13, %f12;
        ld.shared.u8    %rs11, [%r7+20];
        cvt.rn.f32.u16  %f16, %rs11;
        add.s64         %rd15, %rd10, 20;
        ld.global.nc.f32        %f17, [%rd15];
        fma.rn.f32      %f18, %f17, %f16, %f15;
        ld.shared.u8    %rs12, [%r7+36];
        cvt.rn.f32.u16  %f19, %rs12;
        add.s64         %rd16, %rd10, 24;
        ld.global.nc.f32        %f20, [%rd16];
        fma.rn.f32      %f21, %f20, %f19, %f18;
        ld.shared.u8    %rs13, [%r7+37];
        cvt.rn.f32.u16  %f22, %rs13;
        add.s64         %rd17, %rd10, 28;
        ld.global.nc.f32        %f23, [%rd17];
        fma.rn.f32      %f24, %f23, %f22, %f21;
        ld.shared.u8    %rs14, [%r7+38];
        cvt.rn.f32.u16  %f25, %rs14;
        add.s64         %rd18, %rd10, 32;
        ld.global.nc.f32        %f26, [%rd18];
        fma.rn.f32      %f27, %f26, %f25, %f24;
        cvt.rzi.u32.f32         %r27, %f27;
        cvta.to.global.u64      %rd20, %rd3;
        add.s64         %rd21, %rd20, %rd5;
        st.global.u8    [%rd21], %r27;

$L__BB2_12:
        ret;

}